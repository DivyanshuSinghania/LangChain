{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up api and enviornment\n",
    "import os\n",
    "import getpass\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setting Enviorment\n",
    "ENV_PATH = \"/Users/divyanshusinghania/Documents/Github/LangChain/.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "if not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can define what an Document is in the code\n",
    "# Used for in memory operations\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Document Loaders - These are to use to connect and load the data\n",
    "# - Types:\n",
    "# - File (CSV, JSON, EXCEL) Based, Database, API Loader -> Sturctured Data Loader\n",
    "# - File (TEXT, PDF PLUMBER, UNSTRUCTURED PDF), Web Pages -> Unstruvtured Text Loader\n",
    "# - Cloud Based (S3, GCS, AZURE), Enterprise Knowledge Base -> Specialized and cloud\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"/Users/divyanshusinghania/Documents/ML_Learning/Reaserch Papers/2503.11651v1.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGT: Visual Geometry Grounded Transformer\n",
      "Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\n",
      "Christian Rupprecht1 David Novotny2\n",
      "1Visual Geometry Group, University of Oxford 2Meta AI\n",
      "\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-17T01:12:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/divyanshusinghania/Documents/ML_Learning/Reaserch Papers/2503.11651v1.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking/Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model\n",
    "\n",
    "A big thing is that langsmith dont track embeddings by default, i will make changes afterwards for now its not tracking embeddings and vector store (Without Retriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using OpenAI Embedding Model\n",
    "# - \"text-embedding-3-large\" is .13 dollors per million tokens\n",
    "# - \"text-embedding-3-small\" is .02 dollors per million tokens\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 3072\n",
      "\n",
      "[-0.00047509788419120014, 0.028241749852895737, -0.024183286353945732, -0.0052824439480900764, 0.026876045390963554, 0.020614415407180786, -0.00576559454202652, 0.03540525957942009, -0.024505387991666794, 0.021606484428048134]\n"
     ]
    }
   ],
   "source": [
    "Vector_OpenAI_1 = embed_openai.embed_query(all_splits[0].page_content)\n",
    "Vector_OpenAI_2 = embed_openai.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(Vector_OpenAI_1) == len(Vector_OpenAI_2)\n",
    "print(f\"Generated vectors of length {len(Vector_OpenAI_1)}\\n\")\n",
    "print(Vector_OpenAI_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama free (Local Serving, Laptop Intense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setting Up Ollama Locally for LangChain\n",
    "\n",
    "This note provides a quick revision guide to install and configure Ollama on your local machine so you can use the `OllamaEmbeddings` integration with LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Operating System:** macOS, Windows, or Linux (or WSL)\n",
    "- **Python:** Version 3.11 or later\n",
    "- **Ollama Installer:** Download from [Ollama's website](https://ollama.ai/)\n",
    "\n",
    "---\n",
    "\n",
    "### Installation Steps\n",
    "\n",
    "#### 1. Download and Install Ollama\n",
    "\n",
    "- **Download:** Visit [Ollama](https://ollama.ai/) and download the installer for your OS.\n",
    "- **Install:** Follow the provided installation instructions.\n",
    "\n",
    "#### 2. Start the Ollama Server\n",
    "\n",
    "- **Open Terminal/Command Prompt**\n",
    "- Run the command:\n",
    "  ```bash\n",
    "  ollama serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed_ollama = OllamaEmbeddings(\n",
    "    model=\"llama3.2:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 3072\n",
      "\n",
      "[0.015058594, 0.0008505595, 0.015283242, 0.00901549, -0.021684092, -0.014709408, -0.03411613, 0.021666866, -0.0024111453, -0.0067152134]\n"
     ]
    }
   ],
   "source": [
    "Vector_Ollama_1 = embed_ollama.embed_query(all_splits[0].page_content)\n",
    "Vector_Ollama_2 = embed_ollama.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(Vector_Ollama_1) == len(Vector_Ollama_2)\n",
    "print(f\"Generated vectors of length {len(Vector_Ollama_1)}\\n\")\n",
    "print(Vector_Ollama_1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Integration (Clashing with numpy will resolve later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_HF = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_HF_1 = embed_HF.embed_query(all_splits[0].page_content)\n",
    "Vector_HF_2 = embed_HF.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(Vector_HF_1) == len(Vector_HF_2)\n",
    "print(f\"Generated vectors of length {len(Vector_HF_1)}\\n\")\n",
    "print(Vector_HF_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store\n",
    "\n",
    "A big thing is that langsmith dont track embeddings by default, i will make changes afterwards for now its not tracking embeddings and vector store (Without Retriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Langchain self implementation of Vector Store for In Memory\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store_OpenAI = InMemoryVectorStore(embed_openai)\n",
    "vector_store_Ollama = InMemoryVectorStore(embed_ollama)\n",
    "\n",
    "# Just to save some cost on OpenAI API\n",
    "all_splits = all_splits[:2]\n",
    "# len(all_splits)\n",
    "\n",
    "ids_OpenAI = vector_store_OpenAI.add_documents(documents=all_splits)\n",
    "ids_Ollama = vector_store_Ollama.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
