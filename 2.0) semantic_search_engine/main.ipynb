{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up api and enviornment\n",
    "import os\n",
    "import getpass\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setting Enviorment\n",
    "ENV_PATH = \"/Users/divyanshusinghania/Documents/Github/LangChain/.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "if not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can define what an Document is in the code\n",
    "# Used for in memory operations\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Document Loaders - These are to use to connect and load the data\n",
    "# - Types:\n",
    "# - File (CSV, JSON, EXCEL) Based, Database, API Loader -> Sturctured Data Loader\n",
    "# - File (TEXT, PDF PLUMBER, UNSTRUCTURED PDF), Web Pages -> Unstruvtured Text Loader\n",
    "# - Cloud Based (S3, GCS, AZURE), Enterprise Knowledge Base -> Specialized and cloud\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = r\"/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGT: Visual Geometry Grounded Transformer\n",
      "Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\n",
      "Christian Rupprecht1 David Novotny2\n",
      "1Visual Geometry Group, University of Oxford 2Meta AI\n",
      "…\n",
      "Figure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts\n",
      "up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which\n",
      "often outperforms optimization-based alternatives without further processing.\n",
      "Abstract\n",
      "We present VGGT, a feed-forward neural network that di-\n",
      "rectly infers all key 3D attributes of a scene, including cam-\n",
      "era parameters, point maps, depth maps, and 3D point\n",
      "tracks, from one, a few, or hundreds of its views. This\n",
      "approach is a step forward in 3D computer vision, where\n",
      "models have typically been constrained to and special-\n",
      "ized for single tasks. It is also simple and efficient, re-\n",
      "constructing images in under one second, and still out-\n",
      "performing alternatives that require post-processing with\n",
      "visual geometry optimization techniques. The network\n",
      "achieves state-of-the-art results in multiple 3D tasks, in-\n",
      "cluding camera parameter estimation, multi-view depth es-\n",
      "timation, dense point cloud reconstruction, and 3D point\n",
      "tracking. We also show that using pretrained VGGT as a\n",
      "feature backbone significantly enhances downstream tasks,\n",
      "such as non-rigid point tracking and feed-forward novel\n",
      "view synthesis. Code and models are publicly available at\n",
      "https://github.com/facebookresearch/vggt.\n",
      "1. Introduction\n",
      "We consider the problem of estimating the 3D attributes\n",
      "of a scene, captured in a set of images, utilizing a feed-\n",
      "forward neural network. Traditionally, 3D reconstruction\n",
      "has been approached with visual-geometry methods, uti-\n",
      "lizing iterative optimization techniques like Bundle Adjust-\n",
      "ment (BA) [45]. Machine learning has often played an im-\n",
      "portant complementary role, addressing tasks that cannot\n",
      "be solved by geometry alone, such as feature matching and\n",
      "monocular depth prediction. The integration has become\n",
      "increasingly tight, and now state-of-the-art Structure-from-\n",
      "Motion (SfM) methods like VGGSfM [125] combine ma-\n",
      "chine learning and visual geometry end-to-end via differ-\n",
      "entiable BA. Even so, visual geometry still plays a major\n",
      "role in 3D reconstruction, which increases complexity and\n",
      "computational cost.\n",
      "As networks become ever more powerful, we ask if,\n",
      "finally, 3D tasks can be solved directly by a neural net-\n",
      "work, eschewing geometry post-processing almost entirely.\n",
      "Recent contributions like DUSt3R [129] and its evolution\n",
      "1\n",
      "arXiv:2503.11651v1  [cs.CV]  14 Mar 2025\n",
      "\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-17T01:12:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[0].page_content}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking/Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-17T01:12:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1', 'start_index': 0}\n",
      "VGGT: Visual Geometry Grounded Transformer\n",
      "Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\n",
      "Christian Rupprecht1 David Novotny2\n",
      "1Visual Geometry Group, University of Oxford 2Meta AI\n",
      "…\n",
      "Figure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts\n",
      "up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which\n",
      "often outperforms optimization-based alternatives without further processing.\n",
      "Abstract\n",
      "We present VGGT, a feed-forward neural network that di-\n",
      "rectly infers all key 3D attributes of a scene, including cam-\n",
      "era parameters, point maps, depth maps, and 3D point\n",
      "tracks, from one, a few, or hundreds of its views. This\n",
      "approach is a step forward in 3D computer vision, where\n",
      "models have typically been constrained to and special-\n",
      "ized for single tasks. It is also simple and efficient, re-\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].metadata)\n",
    "print(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model\n",
    "\n",
    "A big thing is that langsmith dont track embeddings by default, i will make changes afterwards for now its not tracking embeddings and vector store (Without Retriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using OpenAI Embedding Model\n",
    "# - \"text-embedding-3-large\" is .13 dollors per million tokens\n",
    "# - \"text-embedding-3-small\" is .02 dollors per million tokens\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 1536\n",
      "\n",
      "[-0.027714015915989876, -0.009518314152956009, 0.032930199056863785, -0.026032162830233574, -0.03061460331082344, -0.05264931917190552, 0.02175440639257431, 0.028250260278582573, -0.05869423970580101, 0.01407638005912304]\n"
     ]
    }
   ],
   "source": [
    "Vector_OpenAI_1 = embed_openai.embed_query(all_splits[0].page_content)\n",
    "Vector_OpenAI_2 = embed_openai.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(Vector_OpenAI_1) == len(Vector_OpenAI_2)\n",
    "print(f\"Generated vectors of length {len(Vector_OpenAI_1)}\\n\")\n",
    "print(Vector_OpenAI_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama free (Local Serving, Laptop Intense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setting Up Ollama Locally for LangChain\n",
    "\n",
    "This note provides a quick revision guide to install and configure Ollama on your local machine so you can use the `OllamaEmbeddings` integration with LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Operating System:** macOS, Windows, or Linux (or WSL)\n",
    "- **Python:** Version 3.11 or later\n",
    "- **Ollama Installer:** Download from [Ollama's website](https://ollama.ai/)\n",
    "\n",
    "---\n",
    "\n",
    "### Installation Steps\n",
    "\n",
    "#### 1. Download and Install Ollama\n",
    "\n",
    "- **Download:** Visit [Ollama](https://ollama.ai/) and download the installer for your OS.\n",
    "- **Install:** Follow the provided installation instructions.\n",
    "\n",
    "#### 2. Start the Ollama Server\n",
    "\n",
    "- **Open Terminal/Command Prompt**\n",
    "- Run the command:\n",
    "  ```bash\n",
    "  ollama serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed_ollama = OllamaEmbeddings(\n",
    "    model=\"llama3.2:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 3072\n",
      "\n",
      "[0.016136965, 0.0008328349, 0.016224116, 0.009177097, -0.02200352, -0.015884876, -0.03370971, 0.022160796, -0.0013032064, -0.007980467]\n"
     ]
    }
   ],
   "source": [
    "Vector_Ollama_1 = embed_ollama.embed_query(all_splits[0].page_content)\n",
    "Vector_Ollama_2 = embed_ollama.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(Vector_Ollama_1) == len(Vector_Ollama_2)\n",
    "print(f\"Generated vectors of length {len(Vector_Ollama_1)}\\n\")\n",
    "print(Vector_Ollama_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Integration (Clashing with numpy will resolve later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyanshusinghania/Documents/Github/LangChain/Lang_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_HF = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 768\n",
      "\n",
      "[-0.049434348940849304, 0.02994764968752861, 0.057593680918216705, 0.06480643898248672, -0.004581977613270283, 0.008656955324113369, -0.009640422649681568, -0.03884323313832283, -0.02391975186765194, -0.007224599830806255]\n"
     ]
    }
   ],
   "source": [
    "Vector_HF_1 = embed_HF.embed_query(all_splits[0].page_content)\n",
    "Vector_HF_2 = embed_HF.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(Vector_HF_1) == len(Vector_HF_2)\n",
    "print(f\"Generated vectors of length {len(Vector_HF_1)}\\n\")\n",
    "print(Vector_HF_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store\n",
    "\n",
    "A big thing is that langsmith dont track embeddings by default, i will make changes afterwards for now its not tracking embeddings and vector store (Without Retriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Langchain self implementation of Vector Store for In Memory\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store_OpenAI = InMemoryVectorStore(embed_openai)\n",
    "vector_store_Ollama = InMemoryVectorStore(embed_ollama)\n",
    "vector_store_HF = InMemoryVectorStore(embed_HF)\n",
    "\n",
    "# Just to save some cost on OpenAI API\n",
    "some_splits = all_splits[:2]\n",
    "# len(all_splits)\n",
    "\n",
    "ids_OpenAI = vector_store_OpenAI.add_documents(documents=some_splits)\n",
    "ids_Ollama = vector_store_Ollama.add_documents(documents=some_splits)\n",
    "ids_HF = vector_store_HF.add_documents(documents=some_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-17T01:12:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='VGGT: Visual Geometry Grounded Transformer\\nJianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\\nChristian Rupprecht1 David Novotny2\\n1Visual Geometry Group, University of Oxford 2Meta AI\\n…\\nFigure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts\\nup to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which\\noften outperforms optimization-based alternatives without further processing.\\nAbstract\\nWe present VGGT, a feed-forward neural network that di-\\nrectly infers all key 3D attributes of a scene, including cam-\\nera parameters, point maps, depth maps, and 3D point\\ntracks, from one, a few, or hundreds of its views. This\\napproach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-17T01:12:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1', 'start_index': 794}, page_content='approach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-\\nconstructing images in under one second, and still out-\\nperforming alternatives that require post-processing with\\nvisual geometry optimization techniques. The network\\nachieves state-of-the-art results in multiple 3D tasks, in-\\ncluding camera parameter estimation, multi-view depth es-\\ntimation, dense point cloud reconstruction, and 3D point\\ntracking. We also show that using pretrained VGGT as a\\nfeature backbone significantly enhances downstream tasks,\\nsuch as non-rigid point tracking and feed-forward novel\\nview synthesis. Code and models are publicly available at\\nhttps://github.com/facebookresearch/vggt.\\n1. Introduction\\nWe consider the problem of estimating the 3D attributes\\nof a scene, captured in a set of images, utilizing a feed-\\nforward neural network. Traditionally, 3D reconstruction')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "DB_path = r\"/Users/divyanshusinghania/Documents/Github/LangChain/2.0) semantic_search_engine/chroma_langchain_db\"\n",
    "\n",
    "vector_store_OpenAI = Chroma(\n",
    "    collection_name= \"OpenAI\",\n",
    "    embedding_function= embed_openai,\n",
    "    persist_directory= DB_path,  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store_Ollama = Chroma(\n",
    "    collection_name= \"Ollama\",\n",
    "    embedding_function= embed_ollama,\n",
    "    persist_directory= DB_path,  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store_HF = Chroma(\n",
    "    collection_name= \"HuggingFace\",\n",
    "    embedding_function= embed_HF,\n",
    "    persist_directory= DB_path,  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "ids_OpenAI = vector_store_OpenAI.add_documents(documents=some_splits)\n",
    "ids_Ollama = vector_store_Ollama.add_documents(documents=some_splits)\n",
    "ids_HF = vector_store_HF.add_documents(documents=some_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='VGGT: Visual Geometry Grounded Transformer\n",
      "Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\n",
      "Christian Rupprecht1 David Novotny2\n",
      "1Visual Geometry Group, University of Oxford 2Meta AI\n",
      "…\n",
      "Figure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts\n",
      "up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which\n",
      "often outperforms optimization-based alternatives without further processing.\n",
      "Abstract\n",
      "We present VGGT, a feed-forward neural network that di-\n",
      "rectly infers all key 3D attributes of a scene, including cam-\n",
      "era parameters, point maps, depth maps, and 3D point\n",
      "tracks, from one, a few, or hundreds of its views. This\n",
      "approach is a step forward in 3D computer vision, where\n",
      "models have typically been constrained to and special-\n",
      "ized for single tasks. It is also simple and efficient, re-' metadata={'author': '', 'creationdate': '2025-03-17T01:12:06+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}\n"
     ]
    }
   ],
   "source": [
    "# These are Syncronus\n",
    "\n",
    "results_HF = vector_store_HF.similarity_search(\n",
    "    \"I have given you a Document, i wanted to know what is its name.\"\n",
    ")\n",
    "\n",
    "results_Ollama = vector_store_Ollama.similarity_search(\n",
    "    \"I have given you a Document, i wanted to know what is its name.\"\n",
    ")\n",
    "\n",
    "results_OpenAI = vector_store_OpenAI.similarity_search(\n",
    "    \"I have given you a Document, i wanted to know what is its name.\"\n",
    ")\n",
    "\n",
    "# This is Asyncronus\n",
    "\n",
    "results = await vector_store_OpenAI.asimilarity_search(\"I have given you a Document, i wanted to know what is its name.\")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(id='1309f11c-a775-4f2d-8b7b-2f380ca0a5e6', metadata={'author': '', 'creationdate': '2025-03-17T01:12:06+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='VGGT: Visual Geometry Grounded Transformer\\nJianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\\nChristian Rupprecht1 David Novotny2\\n1Visual Geometry Group, University of Oxford 2Meta AI\\n…\\nFigure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts\\nup to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which\\noften outperforms optimization-based alternatives without further processing.\\nAbstract\\nWe present VGGT, a feed-forward neural network that di-\\nrectly infers all key 3D attributes of a scene, including cam-\\nera parameters, point maps, depth maps, and 3D point\\ntracks, from one, a few, or hundreds of its views. This\\napproach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-'), 1.8712539543926003)\n",
      "1.8712539543926003\n",
      "(Document(id='a2a4d2cf-a374-47a8-a87a-429e272b2cf7', metadata={'author': '', 'creationdate': '2025-03-17T01:12:06+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='VGGT: Visual Geometry Grounded Transformer\\nJianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2\\nChristian Rupprecht1 David Novotny2\\n1Visual Geometry Group, University of Oxford 2Meta AI\\n…\\nFigure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts\\nup to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which\\noften outperforms optimization-based alternatives without further processing.\\nAbstract\\nWe present VGGT, a feed-forward neural network that di-\\nrectly infers all key 3D attributes of a scene, including cam-\\nera parameters, point maps, depth maps, and 3D point\\ntracks, from one, a few, or hundreds of its views. This\\napproach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-'), 1.9146429550390895)\n",
      "1.9146429550390895\n",
      "(Document(id='9d5ccf82-ee7f-430a-94ab-cb8a2293590a', metadata={'author': '', 'creationdate': '2025-03-17T01:12:06+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'start_index': 794, 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='approach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-\\nconstructing images in under one second, and still out-\\nperforming alternatives that require post-processing with\\nvisual geometry optimization techniques. The network\\nachieves state-of-the-art results in multiple 3D tasks, in-\\ncluding camera parameter estimation, multi-view depth es-\\ntimation, dense point cloud reconstruction, and 3D point\\ntracking. We also show that using pretrained VGGT as a\\nfeature backbone significantly enhances downstream tasks,\\nsuch as non-rigid point tracking and feed-forward novel\\nview synthesis. Code and models are publicly available at\\nhttps://github.com/facebookresearch/vggt.\\n1. Introduction\\nWe consider the problem of estimating the 3D attributes\\nof a scene, captured in a set of images, utilizing a feed-\\nforward neural network. Traditionally, 3D reconstruction'), 1.6269913206979643)\n",
      "1.6269913206979643\n"
     ]
    }
   ],
   "source": [
    "results_OpenAI = vector_store_OpenAI.similarity_search_with_score(\n",
    "    \"I have given you a Document, i wanted to know what is its name.\"\n",
    ")\n",
    "_, score = results_OpenAI[0]\n",
    "print(results_OpenAI[0]) # -> 1536 size, paid\n",
    "print(score)\n",
    "\n",
    "results_HF = vector_store_HF.similarity_search_with_score(\n",
    "    \"I have given you a Document, i wanted to know what is its name.\"\n",
    ")\n",
    "_, score = results_HF[0]\n",
    "print(results_HF[0]) # -> 768 size, free\n",
    "print(score)\n",
    "\n",
    "results_Ollama = vector_store_Ollama.similarity_search_with_score(\n",
    "    \"I have given you a Document, i wanted to know what is its name.\"\n",
    ")\n",
    "_, score = results_Ollama[0]\n",
    "print(results_Ollama[0]) # -> 3072 size, free\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to confirm the tracking is going on \n",
    "# but ofcourse the embedding and vectors are not tracked as the are not runnables\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Document(id='7b6019db-c7fc-4735-995b-7ae270b1c8a0', metadata={'author': '', 'creationdate': '2025-03-17T01:12:06+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'start_index': 794, 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='approach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-\\nconstructing images in under one second, and still out-\\nperforming alternatives that require post-processing with\\nvisual geometry optimization techniques. The network\\nachieves state-of-the-art results in multiple 3D tasks, in-\\ncluding camera parameter estimation, multi-view depth es-\\ntimation, dense point cloud reconstruction, and 3D point\\ntracking. We also show that using pretrained VGGT as a\\nfeature backbone significantly enhances downstream tasks,\\nsuch as non-rigid point tracking and feed-forward novel\\nview synthesis. Code and models are publicly available at\\nhttps://github.com/facebookresearch/vggt.\\n1. Introduction\\nWe consider the problem of estimating the 3D attributes\\nof a scene, captured in a set of images, utilizing a feed-\\nforward neural network. Traditionally, 3D reconstruction')],\n",
       " [Document(id='7b6019db-c7fc-4735-995b-7ae270b1c8a0', metadata={'author': '', 'creationdate': '2025-03-17T01:12:06+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-03-17T01:12:06+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/divyanshusinghania/Documents/Github/Reaserch Papers/2503.11651v1.pdf', 'start_index': 794, 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='approach is a step forward in 3D computer vision, where\\nmodels have typically been constrained to and special-\\nized for single tasks. It is also simple and efficient, re-\\nconstructing images in under one second, and still out-\\nperforming alternatives that require post-processing with\\nvisual geometry optimization techniques. The network\\nachieves state-of-the-art results in multiple 3D tasks, in-\\ncluding camera parameter estimation, multi-view depth es-\\ntimation, dense point cloud reconstruction, and 3D point\\ntracking. We also show that using pretrained VGGT as a\\nfeature backbone significantly enhances downstream tasks,\\nsuch as non-rigid point tracking and feed-forward novel\\nview synthesis. Code and models are publicly available at\\nhttps://github.com/facebookresearch/vggt.\\n1. Introduction\\nWe consider the problem of estimating the 3D attributes\\nof a scene, captured in a set of images, utilizing a feed-\\nforward neural network. Traditionally, 3D reconstruction')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# Only will use HF for Explaination (not building an application here)\n",
    "@chain\n",
    "def retrieve_hf(query: str) -> List[Document]:\n",
    "    return vector_store_HF.similarity_search(query, k=1)\n",
    "\n",
    "retrieve_hf.batch(\n",
    "    [\n",
    "        \"query 1\",\n",
    "        \"query 2\",\n",
    "    ],\n",
    ")\n",
    "# -> getting an HuggingFace error saying process forked\n",
    "# -> I think batch gives parllelism to retriver and hence the error\n",
    "\n",
    "\n",
    "\n",
    "# retriever = vector_store_HF.as_retriever(\n",
    "#     search_type=\"similarity\",\n",
    "#     search_kwargs={\"k\": 1},\n",
    "# )\n",
    "\n",
    "# retriever.batch(\n",
    "#     [\n",
    "#         \"query 1\",\n",
    "#         \"query 2\",\n",
    "#     ],\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
