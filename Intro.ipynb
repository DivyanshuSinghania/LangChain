{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start from OpenAI integrations as thats what I have.\n",
    "\n",
    "# Env Setup\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_PATH = \"/Users/divyanshusinghania/Documents/Github/LangChain/.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "# Model Setup Through Langchain's Core library (not langchain_core)\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# model.invoke(\"Hello, world!\")\n",
    "\n",
    "# This Approch is used if we want to make a multi provider app\n",
    "# This helps in integrating different model providers. \n",
    "\n",
    "\n",
    "# Model setup Through Langchain's Integrated library\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# llm.invoke(\"Hello, world!\")\n",
    "\n",
    "# This Is more simplistic and only contains openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensuring tracing is enabled in langsmith\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1: Adding a db_retrieve (just file access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def db_retrive(path):\n",
    "    with open(path, 'r') as file:\n",
    "        luffy_facts = file.read()\n",
    "    return luffy_facts\n",
    "\n",
    "luffy_fact = db_retrive(\"/Users/divyanshusinghania/Documents/Github/LangChain/luffy.text\")\n",
    "\n",
    "system_prompt = f\"You are a person meeting some new people in a bay area, Answer the user query in light piraty tone, here's some info about you: {luffy_fact}\"\n",
    "user_prompt = \"Hello, can you please introduce yourself?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt),\n",
    "    HumanMessage(user_prompt),\n",
    "]\n",
    "\n",
    "# model.invoke(messages)\n",
    "\n",
    "# Output:\n",
    "# AIMessage(content=\"Ahoy, matey! The name's Luffy, cap'n of this aqu√≠ ragtag crew of dreams and daring adventures! I be a pirate of the grand seas, sportin' the power of a rubber man, stretchin' me limbs like a fine seaweed! \\n\\nIn a world where only a few have the might of superpowers, I be proud to sail with me crew, huntin' for treasure, and feasting on the most delectable grub the ocean has to offer! I'm a jolly soul, always laughin' and spreadin' kindness like a gentle breeze. And ye best believe, when it comes to strength, I pack a punch that could rattle the timbers of the fiercest ship!\\n\\nSo tell me, what brings ye to these shores? Ready to join this merry band o' misfits? Arrr!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 103, 'total_tokens': 277, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BI8KFGEOMW3vmE8vmi7E5dd8GIqVU', 'finish_reason': 'stop', 'logprobs': None}, id='run-e75662c0-9d6c-42fb-bc55-b431873cd503-0', usage_metadata={'input_tokens': 103, 'output_tokens': 174, 'total_tokens': 277, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2: Using different prompt for less API usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Setup\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_PATH = \"/Users/divyanshusinghania/Documents/Github/LangChain/.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "# Model Setup Through Langchain's Core library (not langchain_core)\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def db_retrive(path):\n",
    "    with open(path, 'r') as file:\n",
    "        luffy_facts = file.read()\n",
    "    return luffy_facts\n",
    "\n",
    "dog_fact = db_retrive(\"/Users/divyanshusinghania/Documents/Github/LangChain/less_dog.txt\")\n",
    "\n",
    "system_prompt = f\"you are bieng adopted, you have to hide the facts that: {dog_fact}, how will you act out.\"\n",
    "user_prompt = \"hey buddy lets go walking.\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt),\n",
    "    HumanMessage(user_prompt),\n",
    "]\n",
    "\n",
    "# Stream the output chunk by chunk:\n",
    "# for chunk in model.stream(messages):\n",
    "#     print(chunk.content, end=\" | \")\n",
    "\n",
    "# model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example3: Using Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt\n",
    "# Give the ans in ChatPromptValue that still works with model.invoke or stream\n",
    "\n",
    "# prompt.to_messages()\n",
    "# to get the messages\n",
    "# Output: [SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
